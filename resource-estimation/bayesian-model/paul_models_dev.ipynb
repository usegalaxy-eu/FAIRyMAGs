{ "cells": [ { "cell_type": "markdown", "metadata": { "language": "markdown" }, "source": [ "# K-mer -> features -> Random Forest trade-off plot", "", "This notebook loads per-sample k-mer count files, computes summary features, aligns them with the metadata target peak_mem_in_gbs, trains a Random Forest (Model 4) and plots the trade-off between percent underpredicted samples and total excess memory for a few models (including Random Forest).", "", "Notes:", "- Adjust KMERS_FOLDER and METADATA_CSV if your paths differ.", "- The code uses a lightweight Shannon calculation and common summary stats so scikit-bio is not required." ] }, { "cell_type": "code", "metadata": { "language": "python" }, "source": [ "# Uncomment and run this if you need to install packages in a new environment (optional).", "# !pip install numpy pandas scikit-learn matplotlib seaborn joblib scipy" ] }, { "cell_type": "code", "metadata": { "language": "python" }, "source": [ "import os", "import glob", "import numpy as np", "import pandas as pd", "import matplotlib.pyplot as plt", "import seaborn as sns", "from scipy.stats import entropy", "from sklearn.linear_model import LinearRegression", "from sklearn.dummy import DummyRegressor", "from sklearn.ensemble import RandomForestRegressor", "from sklearn.model_selection import KFold, cross_val_predict", "from sklearn.metrics import r2_score", "import joblib", "", "sns.set(style='whitegrid')" ] }, { "cell_type": "code", "metadata": { "language": "python" }, "source": [ "def load_jellyfish_output(path):", " """Load a jellyfish k-mer counts file robustly.\n", " Expects two columns: kmer and count. Handles whitespace or tab separators and optional header lines.\n", " """", " # Try a couple of sensible separators", " for sep in [None, '\t', ' ', ',']:", " try:", " if sep is None:", " # whitespace delimiter", " df = pd.read_csv(path, delim_whitespace=True, header=None, comment='#')", " else:", " df = pd.read_csv(path, sep=sep, header=None, comment='#')", " # Expect at least two columns; first is kmer, second is count.", " if df.shape[1] >= 2:", " df = df.iloc[:, :2]", " df.columns = ['kmer', 'count']", " # ensure counts numeric", " df['count'] = pd.to_numeric(df['count'], errors='coerce').fillna(0).astype(float)", " return df", " except Exception:", " continue", " # If we got here, raise an informative error", " raise ValueError(f"Could not parse k-mer counts file: {path}")", "", "def compute_stats_from_counts_file(path):", " """Compute a compact set of k-mer derived features from a jellyfish dump file path."""", " df = load_jellyfish_output(path)", " counts = df['count'].values.astype(float)", " total_count = counts.sum()", " # Unique k-mers (count > 0)", " unique_kmers = int((counts > 0).sum())", " mean_count = float(np.mean(counts)) if counts.size else 0.0", " median_count = float(np.median(counts)) if counts.size else 0.0", " std_count = float(np.std(counts)) if counts.size else 0.0", " max_count = float(counts.max()) if counts.size else 0.0", " min_count = float(counts.min()) if counts.size else 0.0", " count_range = max_count - min_count", " num_singletons = int((counts == 1).sum())", " num_doubletons = int((counts == 2).sum())", " percent_singletons = (num_singletons / unique_kmers * 100) if unique_kmers > 0 else 0.0", " # Shannon: compute on relative frequencies across observed k-mers (counts>0)", " positive = counts[counts > 0]", " if positive.size:", " p = positive / positive.sum()", " shannon = float(entropy(p, base=np.e))", " else:", " shannon = 0.0", "", " return {", " 'total_count': total_count,", " 'unique_kmers': unique_kmers,", " 'mean_count': mean_count,", " 'median_count': median_count,", " 'std_count': std_count,", " 'count_range': count_range,", " 'num_singletons': num_singletons,", " 'num_doubletons': num_doubletons,", " 'percent_singletons': percent_singletons,", " 'shannon': shannon,", " }" ] }, { "cell_type": "code", "metadata": { "language": "python" }, "source": [ "# Parameters - update if needed", "KMERS_FOLDER = "input/subset_3.15.3_metaspades/5mers"", "METADATA_CSV = "input/updated_mgnify_assemblies_stats_v3.15.3_metaspades_subset.csv"", "", "# Gather files", "kmer_files = sorted([p for p in glob.glob(os.path.join(KMERS_FOLDER, '*')) if os.path.isfile(p)])", "print(f"Found {len(kmer_files)} k-mer files in {KMERS_FOLDER}")", "", "# Compute features per sample", "records = []", "for p in kmer_files:", " sample = os.path.splitext(os.path.basename(p))[0]", " try:", " feats = compute_stats_from_counts_file(p)", " except Exception as e:", " print(f"Skipping {sample} because of parse error: {e}")", " continue", " # file size in GB", " try:", " file_size_gb = os.path.getsize(p) / (1024 ** 3)", " except Exception:", " file_size_gb = np.nan", " feats['file_size_gb'] = float(file_size_gb)", " feats['sample'] = sample", " records.append(feats)", "", "if not records:", " raise SystemExit("No feature records computed - check k-mer files path and formats.")", "", "stats_df = pd.DataFrame.from_records(records).set_index('sample')", "print('Computed features shape:', stats_df.shape)", "display(stats_df.head())" ] }, { "cell_type": "code", "metadata": { "language": "python" }, "source": [ "# Load metadata and align with features", "merged_df = pd.read_csv(METADATA_CSV)", "if 'srr_id' not in merged_df.columns or 'peak_mem_in_gbs' not in merged_df.columns:", " print('Metadata columns:', merged_df.columns.tolist())", " raise SystemExit('Metadata CSV missing required columns srr_id and/or peak_mem_in_gbs.')", "", "y_df = merged_df.set_index('srr_id')[['peak_mem_in_gbs']].copy()", "", "# Intersection of sample names in k-mer filenames and srr_id in metadata", "common = stats_df.index.intersection(y_df.index)", "print(f"Common samples between features and metadata: {len(common)}")", "if len(common) == 0:", " # show a few sample names to help debug", " print('First feature sample names:', list(stats_df.index[:10]))", " print('First metadata srr_id examples:', list(y_df.index[:10]))", " raise SystemExit('No common sample IDs found; ensure k-mer filenames are SRR IDs or update mapping.')", "", "stats_X = stats_df.loc[common].sort_index()", "y = y_df.loc[common, 'peak_mem_in_gbs'].sort_index()", "", "# Fill NaNs reasonably", "stats_X = stats_X.fillna(0)", "y = y.fillna(y.median())", "", "print('Final X shape:', stats_X.shape)", "print('Final y shape:', y.shape)" ] }, { "cell_type": "code", "metadata": { "language": "python" }, "source": [ "# Define model set (we will highlight Random Forest / Model 4 among this list)", "features_1 = ['std_count']", "features_2 = ['file_size_gb']", "features_all = stats_X.columns.tolist()", "", "models = [", " ("Model 1: Std. kmer count (Linear)", features_1, LinearRegression()),", " ("Model 2: File size (Linear)", features_2, LinearRegression()),", " ("Model 3: Predict 250 (Dummy)", features_all, DummyRegressor(strategy='constant', constant=250)),", " ("Model 4: All features (Random Forest)", features_all, RandomForestRegressor(random_state=42)),", "]", "", "cv = KFold(n_splits=5, shuffle=True, random_state=42)", "", "# Calculate cross-validated predictions and the trade-off curve for each model", "adjustment_steps = np.arange(-100, 101, 5) # GB offsets to shift predictions by", "", "plt.figure(figsize=(12, 8))", "for title, feat_list, model in models:", " X = stats_X[feat_list]", " # cross-validated predictions (keeps predictions out-of-fold)", " print(f'Computing CV predictions for: {title} with {len(feat_list)} features...')", " y_pred_base = cross_val_predict(model, X, y, cv=cv)", "", " underpred_percent_list = []", " excess_memory_list = []", " n_samples = len(y)", "", " # For each possible additive adjustment, compute percent underpred and total excess memory (GB)", " for adj in adjustment_steps:", " y_pred_adj = y_pred_base + adj", " under_pred_mask = y_pred_adj < y.values", " under_pred_count = np.sum(under_pred_mask)", " under_pred_percent = (under_pred_count / n_samples) * 100", " # For predictions >= actual, compute how much extra memory would be allocated", " excess_memory = y_pred_adj[<del>under_pred_mask] - y.values[</del>under_pred_mask]", " total_excess_memory = np.sum(excess_memory) if excess_memory.size else 0.0", " underpred_percent_list.append(under_pred_percent)", " excess_memory_list.append(total_excess_memory)", "", " plt.plot(underpred_percent_list, excess_memory_list, label=title, marker='o')", "", "plt.xlabel('Percentage of Underpredicted Samples (%)')", "plt.ylabel('Total Excess Memory (GB)')", "plt.title('Trade-off Between Underpredicted Samples and Excess Memory Across Models')", "plt.legend()", "plt.grid(True)", "plt.xlim(0, 40)", "plt.ylim(0, max(plt.ylim()[1], 1))", "plt.show()" ] }, { "cell_type": "code", "metadata": { "language": "python" }, "source": [ "# Fit final Random Forest on full data and save the model for later use", "rf_full = RandomForestRegressor(random_state=42)", "rf_full.fit(stats_X[features_all], y)", "joblib.dump(rf_full, 'rf_all_features.joblib')", "print('Saved Random Forest to rf_all_features.joblib')", "", "# Quick diagnostic: R^2 using cross-validated predictions for Random Forest", "y_pred_rf_cv = cross_val_predict(RandomForestRegressor(random_state=42), stats_X[features_all], y, cv=cv)", "print('Random Forest CV R^2 =', r2_score(y, y_pred_rf_cv))", "", "# Show feature importances (if available)", "if hasattr(rf_full, 'feature_importances_'):", " fi = pd.Series(rf_full.feature_importances_, index=features_all).sort_values(ascending=False)", " print('\nTop 10 feature importances:')", " display(fi.head(10))" ] }, { "cell_type": "markdown", "metadata": { "language": "markdown" }, "source": [ "## Next steps / notes", "", "- If some samples are missing from the metadata, check whether your k-mer filenames match srr_id or provide a mapping.", "- You can expand features (e.g., hashed k-mer vectors) and re-run the Random Forest. For very high-dimensional hashed features use a separate script and consider dimensionality reduction before RF.", "- If you want the full original multi-model plot that includes XGBoost/LightGBM, add those models to the models list (requires xgboost/lightgbm installed)." ] } ], "metadata": { "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" }, "language_info": { "name": "python" } }, "nbformat": 4, "nbformat_minor": 5 }
